{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Copy of Copy of CNN_classifier.ipynb","provenance":[{"file_id":"1KC3Fhe0ql9cN7NlAE4sYNiUJ2bbgzpTw","timestamp":1609601258600},{"file_id":"https://github.com/ClaudiaD97/CV_project/blob/main/CNN_classifier.ipynb","timestamp":1609528321840}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"U9mY72YkOanp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609581346619,"user_tz":-60,"elapsed":1874,"user":{"displayName":"Claudia Dorigo","photoUrl":"","userId":"05215363351820162600"}},"outputId":"6f2e253c-7761-40a3-b421-1410cbcca5da"},"source":["# link colab and drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":112,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jOM9u5i2Ga_a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609581349430,"user_tz":-60,"elapsed":629,"user":{"displayName":"Claudia Dorigo","photoUrl":"","userId":"05215363351820162600"}},"outputId":"08c1a0f1-f080-4c08-b4f2-aa6b9c210c38"},"source":["# CV project\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","import pandas as pd\n","import os\n","import torchvision\n","from torchvision import datasets, transforms, models\n","from IPython import display\n","import shelve\n","from PIL import Image\n","import glob\n","import matplotlib\n","from matplotlib.pyplot import *\n","\n","# if available use GPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print('Device: {}'.format(device))\n","\n","trainPath = '/content/drive/My Drive/Units/CV_project/Images/train'\n","testPath =  '/content/drive/My Drive/Units/CV_project/Images/test'\n","dataTrain = torchvision.datasets.ImageFolder(trainPath)\n","dataTest = torchvision.datasets.ImageFolder(testPath)\n","print(dataTrain.classes)\n","print(dataTrain.imgs[99][0])\n","print(len(dataTrain.imgs[99][0]))\n","im = Image.open(dataTrain.imgs[1][0])\n","%matplotlib inline\n","im\n","ii=torch.from_numpy(np.asarray(im))\n","ii"],"execution_count":113,"outputs":[{"output_type":"stream","text":["Device: cpu\n","['Bedroom', 'Coast', 'Forest', 'Highway', 'Industrial', 'InsideCity', 'Kitchen', 'LivingRoom', 'Mountain', 'Office', 'OpenCountry', 'Store', 'Street', 'Suburb', 'TallBuilding']\n","/content/drive/My Drive/Units/CV_project/Images/train/Bedroom/image_0216.jpg\n","76\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[208, 183, 181,  ..., 111, 111, 110],\n","        [207, 183, 181,  ..., 111, 111, 110],\n","        [208, 184, 181,  ..., 112, 112, 110],\n","        ...,\n","        [101, 101,  99,  ...,  28,  28,  29],\n","        [ 96,  93,  93,  ...,  24,  25,  26],\n","        [ 98,  96,  98,  ...,  25,  25,  24]], dtype=torch.uint8)"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"code","metadata":{"id":"tqSMgFSNj-K6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609586223955,"user_tz":-60,"elapsed":623,"user":{"displayName":"Claudia Dorigo","photoUrl":"","userId":"05215363351820162600"}},"outputId":"f2a2f258-6780-4bff-cfe2-09a2d2b4da4a"},"source":["from torch.utils.data import Dataset, Sampler, DataLoader, SubsetRandomSampler\n","\n","class customDataset(Dataset):\n","  '''\n","    Our dataset is a list of lists: [path_to_image, class]\n","  '''\n","\n","  def __init__(self, root_path, transform):\n","    '''\n","      This function takes in input the path of the folder containing class folders,\n","      the tranformation to be applied to each image and creates the dataset\n","    '''\n","    self.data = torchvision.datasets.ImageFolder(root_path)\n","    #self.datasource = torchvision.datasets.ImageFolder(root_path)\n","    #self.data = []\n","    #for i in range(len(self.datasource)):\n","    #  true_class = self.datasource[i][1]\n","    #  im = Image.open(self.datasource.imgs[i][0])\n","    #  img_tensor = transform(im)\n","    #  self.data.append([img_tensor,true_class])\n","    \n","  def __getitem__(self, key):\n","    '''\n","      This function access the image, transforms it and returns: the image as \n","      a tensor and the class (tensor of 15 with 1 in correct index)\n","\n","      Or is it better that the dataset contains already read images like now?\n","    '''\n","    true_class = self.data[key][1]\n","    \n","    im = Image.open(self.data.imgs[key][0])\n","    img_tensor = transform(im)\n","    return img_tensor, true_class\n","    #return self.data[key]\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def get_keys(self):\n","    '''\n","      I need this function in the sampler. But keys are just numbers 0:(len-1)\n","      probably I don't need this function\n","    '''\n","    return range(len(self.data))\n","\n","# just to tensor, i can resize, force to double, normalize, crop, ....\n","def transform(img):\n","  t = transforms.Resize([64,64],interpolation=Image.BILINEAR)\n","  i = t(img)\n","  i = torch.from_numpy(np.asarray(i))\n","  i.unsqueeze_(0)\n","  #i = i.type(torch.cuda.FloatTensor)\n","  return i\n","\n","trainingData = customDataset(trainPath,transform)\n","x = trainingData[1][0]\n","print(x.shape, x.type())\n","print(x)\n","\n","l = trainingData.get_keys()\n","l[657]"],"execution_count":174,"outputs":[{"output_type":"stream","text":["torch.Size([1, 64, 64]) torch.ByteTensor\n","tensor([[[186, 179, 177,  ..., 116, 114, 111],\n","         [187, 179, 177,  ..., 118, 116, 113],\n","         [189, 180, 178,  ..., 119, 117, 115],\n","         ...,\n","         [101,  92,  85,  ...,  30,  28,  27],\n","         [100,  93,  84,  ...,  28,  28,  28],\n","         [ 96,  87,  81,  ...,  29,  27,  26]]], dtype=torch.uint8)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["657"]},"metadata":{"tags":[]},"execution_count":174}]},{"cell_type":"code","metadata":{"id":"IwwOKL-aszpa","executionInfo":{"status":"ok","timestamp":1609596660279,"user_tz":-60,"elapsed":578,"user":{"displayName":"Claudia Dorigo","photoUrl":"","userId":"05215363351820162600"}}},"source":["def split(dataset, val_size):\n","  '''\n","    @dataset: a customDataset object\n","    @val_size: percentage of the dataset that should compose the validation set\n","\n","    This function allows us to split our dataset into \n","    a validation set and a training set. This is used internally in \n","    Loader\n","  '''\n","\n","  # We want to split our dataset given itself and the % of sample for validation\n","  # or do not shuffle and sample index from uniform (so almost same proportion train-val for each class since ordered)\n","  num = len(dataset)\n","  index = list(dataset.get_keys())\n","  np.random.shuffle(index) # pick at random\n","  #indexes = np.random.random_integers(0,num-1,int(0.85*num))\n","  flag_split = int(val_size * num)\n","\n","  train_index = index[flag_split:]\n","  validation_index = index[:flag_split]\n","\n","  # https://pytorch.org/docs/stable/data.html -> Samples elements randomly from a given list of indices, without replacement\n","  train_sampler = SubsetRandomSampler(train_index)\n","  validation_sampler = SubsetRandomSampler(validation_index)\n","\n","  return train_sampler, validation_sampler\n","\n","\n","def loaders(dataset, val_size, batch_size, num_workers):\n","  ''' \n","    @dataset: a customDataset object\n","    @val_size: the percentage (in [0,1]) of the validation set data\n","    @batch_size: the number of data in each batch\n","    @num_workers: number of subprocesses to use in the data loader\n","  '''\n","\n","  train_sampler, validation_sampler = split(dataset, val_size)\n","  train_loader = DataLoader(dataset,\n","                            batch_size = batch_size,\n","                            sampler = train_sampler,\n","                            num_workers = num_workers)\n","  val_loader = DataLoader(dataset,\n","                          batch_size = batch_size,\n","                          sampler = validation_sampler,\n","                          num_workers = num_workers)\n","  return train_loader, val_loader"],"execution_count":202,"outputs":[]},{"cell_type":"code","metadata":{"id":"MAEjBzRYt1RY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609601097980,"user_tz":-60,"elapsed":645,"user":{"displayName":"Claudia Dorigo","photoUrl":"","userId":"05215363351820162600"}},"outputId":"a7da19e3-1d36-46eb-d683-e89e4bcdb66b"},"source":["import torch.nn.functional as F\n","\n","# now I can try to build a cnn\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.input_dim = 1 * 64 * 64\n","        self.n_classes = 15\n","        \n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1)\n","        #self.conv1.weight = torch.nn.init.normal_(self.conv1.weight, mean=0.0, std=0.01)\n","        \n","        self.maxpooling = nn.MaxPool2d(kernel_size=2,stride=2)\n","        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1)\n","        #self.conv2.weight = torch.nn.init.normal_(self.conv2.weight, mean=0.0, std=0.01)\n","        \n","        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1)\n","        #self.conv3.weight = torch.nn.init.normal_(self.conv3.weight, mean=0.0, std=0.01)\n","        \n","        self.fc1 = nn.Linear(12*12*32,15) # 12*12*32 no padding default\n","       # self.fc1.weight = torch.nn.init.normal_(self.fc1.weight, mean=0.0, std=0.01)\n","        \n","        \n","        \n","        \n","    def forward(self, x, verbose=False):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.maxpooling(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.maxpooling(x)\n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        #print(x.shape) # here is m x 32 x 12 x 12\n","        # but it's transformed in (mx32m12) x 12 instead I want m x (32x12x12)\n","        x = x.view(x.size(0), -1)\n","        #print(x.shape)\n","        x = self.fc1(x)\n","        #x = F.softmax(x, dim=1) # tensor is mx15 I want softmax according to dim 1\n","        return x\n","\n","        \n","net = CNN()\n","for param in net.parameters():\n","    param.requires_grad = True\n","net.to(device) # so I put the model on GPU\n","print(net)\n","\n","#print(net.conv1.weight)"],"execution_count":245,"outputs":[{"output_type":"stream","text":["CNN(\n","  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n","  (maxpooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=4608, out_features=15, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5m1Xz0g3jsp4","outputId":"49be254c-e0b4-40b6-ef05-62cfe59a406d"},"source":["# train and test sets\n","# just to tensor, i can resize, force to double, normalize, crop, ....\n","def transform(img):\n","  t = transforms.Resize([64,64],interpolation=Image.BILINEAR)\n","  i = t(img)\n","  i = torch.from_numpy(np.asarray(i)/255)\n","  i.unsqueeze_(0)\n","  return i\n","\n","trainingSet = customDataset(trainPath, transform)\n","testSet = customDataset(testPath, transform)\n","\n","\n","# train and validation dataloaders\n","batch_size = 32\n","num_workers = 1\n","trainLoader, valLoader = loaders(trainingSet, 0.15, batch_size, num_workers)\n","\n","lr = 0.01\n","momentum = 0.9\n","epochs = 20\n","\n","n_batches = len(trainLoader)\n","print(n_batches)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n","\n","net.train() \n","for e in range(epochs):\n","    correct_classified = 0\n","    for i, data in enumerate(trainLoader):\n","        \n","        batch = data[0].to(device)\n","        batch = batch.float()\n","        labels = data[1].to(device)    \n","\n","\n","        optimizer.zero_grad() \n","        outputs = net(batch)\n","        loss = criterion(outputs, labels) # here I'm not computing the right quantity!\n","        # outputs is mx15 but labels just 15, I need each label to be a vector \n","        predicted_class = torch.argmax(outputs, dim=1)\n","        correct_classified = correct_classified + sum((predicted_class==labels).int())\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        if i % 5 == 0:\n","            print(\"[EPOCH]: {}, [BATCH]: {}/{}, [TRAINING LOSS]: {}\".format(e, i, n_batches, loss.item()))\n","            #display.clear_output(wait=True)\n","    # for now print accuracy at each epoch\n","    print('[TRAINING ACCURACY AT EPOCH {}]: {}'.format(e,correct_classified/(n_batches*batch_size)))\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["40\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: This function is deprecated. Please call randint(0, 1499 + 1) instead\n","  app.launch_new_instance()\n"],"name":"stderr"},{"output_type":"stream","text":["[EPOCH]: 0, [BATCH]: 0/40, [TRAINING LOSS]: 2.461299419403076\n","[EPOCH]: 0, [BATCH]: 5/40, [TRAINING LOSS]: 2.064401626586914\n","[EPOCH]: 0, [BATCH]: 10/40, [TRAINING LOSS]: 1.6929833889007568\n","[EPOCH]: 0, [BATCH]: 15/40, [TRAINING LOSS]: 1.370700478553772\n","[EPOCH]: 0, [BATCH]: 20/40, [TRAINING LOSS]: 1.6902341842651367\n","[EPOCH]: 0, [BATCH]: 25/40, [TRAINING LOSS]: 1.3244773149490356\n","[EPOCH]: 0, [BATCH]: 30/40, [TRAINING LOSS]: 1.239205241203308\n","[EPOCH]: 0, [BATCH]: 35/40, [TRAINING LOSS]: 1.4019213914871216\n","[TRAINING ACCURACY AT EPOCH 0]: 0.598437488079071\n","[EPOCH]: 1, [BATCH]: 0/40, [TRAINING LOSS]: 0.5575956106185913\n","[EPOCH]: 1, [BATCH]: 5/40, [TRAINING LOSS]: 0.8753030300140381\n","[EPOCH]: 1, [BATCH]: 10/40, [TRAINING LOSS]: 0.8142128586769104\n","[EPOCH]: 1, [BATCH]: 15/40, [TRAINING LOSS]: 0.6829840540885925\n","[EPOCH]: 1, [BATCH]: 20/40, [TRAINING LOSS]: 0.8562234044075012\n","[EPOCH]: 1, [BATCH]: 25/40, [TRAINING LOSS]: 1.0574771165847778\n","[EPOCH]: 1, [BATCH]: 30/40, [TRAINING LOSS]: 0.5190238356590271\n","[EPOCH]: 1, [BATCH]: 35/40, [TRAINING LOSS]: 0.8383222222328186\n","[TRAINING ACCURACY AT EPOCH 1]: 0.7789062261581421\n","[EPOCH]: 2, [BATCH]: 0/40, [TRAINING LOSS]: 0.4019526541233063\n","[EPOCH]: 2, [BATCH]: 5/40, [TRAINING LOSS]: 0.38583531975746155\n","[EPOCH]: 2, [BATCH]: 10/40, [TRAINING LOSS]: 0.18760202825069427\n","[EPOCH]: 2, [BATCH]: 15/40, [TRAINING LOSS]: 0.27561089396476746\n","[EPOCH]: 2, [BATCH]: 20/40, [TRAINING LOSS]: 0.581691324710846\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SNLZ1QBpA6t","executionInfo":{"status":"ok","timestamp":1609600155384,"user_tz":-60,"elapsed":592,"user":{"displayName":"Claudia Dorigo","photoUrl":"","userId":"05215363351820162600"}},"outputId":"4ab7c76d-1acd-4e85-be42-d97e2982d863"},"source":["print(40*32)\n","1500*0.85"],"execution_count":237,"outputs":[{"output_type":"stream","text":["1280\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1275.0"]},"metadata":{"tags":[]},"execution_count":237}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUD3-0YOmvdI","executionInfo":{"status":"ok","timestamp":1609599507127,"user_tz":-60,"elapsed":643,"user":{"displayName":"Claudia Dorigo","photoUrl":"","userId":"05215363351820162600"}},"outputId":"283c452f-3c70-433b-adf6-7a24799c8cdb"},"source":["net.conv1.weight"],"execution_count":226,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[[[ 0.0126, -0.0148,  0.0014],\n","          [-0.0205, -0.0042, -0.0003],\n","          [ 0.0153,  0.0148, -0.0039]]],\n","\n","\n","        [[[ 0.0120,  0.0092,  0.0069],\n","          [ 0.0054,  0.0034,  0.0168],\n","          [ 0.0155, -0.0031, -0.0109]]],\n","\n","\n","        [[[-0.0053,  0.0023,  0.0136],\n","          [-0.0067, -0.0041, -0.0028],\n","          [-0.0050,  0.0128,  0.0011]]],\n","\n","\n","        [[[-0.0166, -0.0088, -0.0125],\n","          [-0.0199, -0.0196,  0.0011],\n","          [ 0.0099, -0.0140, -0.0018]]],\n","\n","\n","        [[[ 0.0018,  0.0061,  0.0061],\n","          [-0.0103, -0.0104,  0.0138],\n","          [-0.0128, -0.0068, -0.0085]]],\n","\n","\n","        [[[-0.0019,  0.0147,  0.0006],\n","          [-0.0026,  0.0089,  0.0234],\n","          [ 0.0119,  0.0327, -0.0014]]],\n","\n","\n","        [[[ 0.0051,  0.0067,  0.0107],\n","          [-0.0003,  0.0154, -0.0010],\n","          [ 0.0076,  0.0110,  0.0215]]],\n","\n","\n","        [[[-0.0127,  0.0022, -0.0094],\n","          [ 0.0002, -0.0086, -0.0109],\n","          [-0.0033,  0.0071,  0.0067]]]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":226}]}]}