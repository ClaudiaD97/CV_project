{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just if you run on colab\n",
    "# link colab and drive\n",
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "# CV project\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "import glob\n",
    "import matplotlib\n",
    "from matplotlib.pyplot import *\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from cvxopt import matrix, solvers\n",
    "torch.manual_seed(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our own dataset and dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "  def __init__(self, rootPath, resize_transformation, data_augmentation_transformation=None, augment=True):\n",
    "    ''' \n",
    "      @rootPath: path of the folder containing class subfolders\n",
    "      @transformation: transformation to be applied to each image\n",
    "      @data_augmentation: transformations to be applied to train images\n",
    "      @augment: boolean indicating if data augmentation should be performed\n",
    "    '''\n",
    "    self.data = torchvision.datasets.ImageFolder(rootPath)\n",
    "    self.transformation = resize_transformation\n",
    "    self.data_augmentation = data_augmentation_transformation\n",
    "    self.augment = True\n",
    "    \n",
    "  def __getitem__(self, key):\n",
    "    \n",
    "    true_class = self.data[key][1]\n",
    "    im = Image.open(self.data.imgs[key][0])\n",
    "    if self.augment and self.data_augmentation!=None:\n",
    "      img_tensor = self.data_augmentation(im)\n",
    "    else:\n",
    "      img_tensor = self.transformation(im)\n",
    "\n",
    "    return img_tensor, true_class\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def set_augment(self, value):\n",
    "    self.augment = value\n",
    "\n",
    "  def change_transformation(self, new_transformation):\n",
    "    self.transformation = new_transformation\n",
    "\n",
    "  def change_augmentation(self, new_augmentation):\n",
    "    self.data_augmentation = new_augmentation\n",
    "\n",
    "  def get_keys(self):\n",
    "    return range(len(self.data))\n",
    "\n",
    "def split(dataset, val_size):\n",
    "    '''\n",
    "    @ dataset: a customDataset object\n",
    "    @ val_size: % of validation data\n",
    "    '''\n",
    "    index = list(dataset.get_keys())\n",
    "    val_per_class = int(val_size*100)\n",
    "    validation_index = []\n",
    "    for i in range(15):\n",
    "        idx = random.sample(range(100*i,100*(i+1)),val_per_class)\n",
    "        validation_index= validation_index + idx\n",
    "    train_index = list(set(index)-set(validation_index))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_index)\n",
    "    validation_sampler = SubsetRandomSampler(validation_index)\n",
    "\n",
    "    return train_sampler, validation_sampler\n",
    "\n",
    "def loaders(dataset, val_size, batch_size, num_workers):\n",
    "  ''' \n",
    "    @dataset: a customDataset object\n",
    "    @val_size: % validation data\n",
    "    @batch_size: the number of examples in each batch\n",
    "    @num_workers: number of subprocesses to use in the data loader\n",
    "  '''\n",
    "\n",
    "  train_sampler, validation_sampler = split(dataset, val_size)\n",
    "  train_loader = DataLoader(dataset,\n",
    "                            batch_size = batch_size,\n",
    "                            sampler = train_sampler,\n",
    "                            num_workers = num_workers)\n",
    "  val_loader = DataLoader(dataset,\n",
    "                          batch_size = int(val_size*len(dataset)),\n",
    "                          sampler = validation_sampler,\n",
    "                          num_workers = num_workers)\n",
    "  return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_with_scheduler(net, trainLoader,valLoader, optimizer, criterion, val_patience, validate_each, scheduler=None):\n",
    "\n",
    "    # here I save loss and accuracy\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_accuracy = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    # early stopping\n",
    "    best_net = copy.deepcopy(net.state_dict())\n",
    "    best_loss = 100.0\n",
    "    worsening_count = 0\n",
    "\n",
    "    net.train()\n",
    "    n_batches = len(trainLoader)\n",
    "    for e in range(epochs):\n",
    "        correct_classified = 0\n",
    "        for i, data in enumerate(trainLoader):\n",
    "\n",
    "            batch = data[0].to(device)\n",
    "            batch = batch.float()\n",
    "            labels = data[1].to(device)    \n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            outputs = net(batch)\n",
    "\n",
    "            loss = criterion(outputs, labels) \n",
    "            train_loss.append(loss.item())\n",
    "            predicted_class = torch.argmax(outputs, dim=1)\n",
    "            correct_classified = correct_classified + sum((predicted_class==labels).int())\n",
    "            acc = sum((predicted_class==labels).int())/batch.shape[0]\n",
    "            train_accuracy.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # parameter to decide how often to validate\n",
    "            if i % validate_each ==0:\n",
    "                with torch.no_grad():\n",
    "                    trainingSet.set_augment(False)\n",
    "                    valLoss, valAcc = validate(valLoader,net,criterion)\n",
    "                    # save validation loss and accuracy\n",
    "                    val_loss.append(valLoss)\n",
    "                    val_accuracy.append(valAcc)\n",
    "                    trainingSet.set_augment(True)\n",
    "\n",
    "                # if validation loss increase (at least +1%), increase the counter\n",
    "                if valLoss>best_loss:\n",
    "                    worsening_count = worsening_count+1\n",
    "                    # if I exceed the patience, early stop\n",
    "                    if worsening_count > val_patience:\n",
    "                        return [best_net, train_loss, val_loss, train_accuracy, val_accuracy]\n",
    "                # else reset the counter and use actual validation loss as reference, save the net\n",
    "                else:\n",
    "                    worsening_count = 0\n",
    "                    best_loss = valLoss\n",
    "                    best_net = copy.deepcopy(net.state_dict())\n",
    "\n",
    "                print(\"[LR]: {:.4f}\\n\".format(scheduler.get_last_lr()[0]))\n",
    "                print(\"[EPOCH]: {}, [BATCH]: {}/{}, [LOSS]: t {}, v {},\\t [ACC.]: t {},\\t v {}\".format(e, i, n_batches, loss.item(), valLoss, acc, valAcc))\n",
    "\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    return [best_net, train_loss, val_loss, train_accuracy, val_accuracy]\n",
    "\n",
    "# validation \n",
    "def validate(valLoader, net, criterion):\n",
    "  correct_count=0\n",
    "  size = 0\n",
    "  for i, data in enumerate(valLoader):\n",
    "    batch = data[0].to(device)\n",
    "    batch = batch.float()\n",
    "    labels = data[1].to(device)    \n",
    "\n",
    "    outputs = net(batch)\n",
    "    loss = criterion(outputs, labels) \n",
    "    predicted_class = torch.argmax(outputs, dim=1)\n",
    "    correct_count = correct_count + sum((predicted_class==labels).int())\n",
    "    size = size + batch.shape[0]\n",
    "  acc = correct_count/size\n",
    "  return [loss.item(), acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
